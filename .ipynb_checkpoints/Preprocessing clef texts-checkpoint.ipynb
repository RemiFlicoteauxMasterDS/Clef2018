{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing Clef 2018 texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries and specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils_ import *\n",
    "from FeatureExtractor import *\n",
    "fe=FeatureExtractor()\n",
    "tp=fe.text_preprocessing\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###CIM\n",
    "sentences=[]\n",
    "icd=[]\n",
    "\n",
    "with open('dictionaries/LIBCIM10MULTI.TXT', 'r', encoding=\"latin1\") as f:\n",
    "        \n",
    "        next(f)\n",
    "        \n",
    "        for line in f:\n",
    "            line_=line.split('|')\n",
    "            c=line_[0].replace(' ','')\n",
    "            type_=line_[1]\n",
    "            if type_!='3':\n",
    "                icd.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "th_base_sentences=[]\n",
    "th_codes=[]\n",
    "th_source=[]\n",
    "with open('dictionaries/clef2018_thesaurus.csv', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line_=line.split(';')\n",
    "        th_base_sentences.append(line_[0])\n",
    "        th_codes.append(line_[1])\n",
    "        th_source.append(line_[2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216110\n"
     ]
    }
   ],
   "source": [
    "print(len(th_base_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify compound words\n",
    "Coumpound words will be treated as separated words, but due to spelling habits of each author of the documents we enrich dictionary with a all tied up forms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pc=punctuation.replace('-','')\n",
    "## Composed words (separate by -) will be treat as 2 single words\n",
    "th_base_tokens=[tp.simple_tokenizer(s,pc) for s in th_base_sentences]\n",
    "w=set([w for s in th_base_tokens for w in s if re.search(r'-',w)\n",
    "       and len(w)>3 \n",
    "       and not re.search('[\\s,.\\+/^]',w)\n",
    "       and w not in ['anti-corps']])\n",
    "\n",
    "rev_comp_words={}\n",
    "comp_words=[]\n",
    "for w_ in w:\n",
    "    nw=w_.replace('-','')\n",
    "    if len(nw)>3:\n",
    "        rev_comp_words[nw]= w_\n",
    "        comp_words.append(w_.replace('-',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## fit preprocessing functions with cim 10 entries to build a reference dictionnary\n",
    "th_base_tokens=[tp.simple_tokenizer(s) for s in th_base_sentences]\n",
    "vocab, rev_vocab = build_vocabulary(th_base_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp.comp_words=comp_words\n",
    "tp.rev_comp_words=rev_comp_words\n",
    "corrector_dict=[w for w in vocab]+comp_words # build a more complete dictionary for word recogntion and spelling correction\n",
    "tp.fit_corrector(corrector_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "for file in ['AlignedCauses_2006-2012full.csv','AlignedCauses_2013full.csv','AlignedCauses_2014_full.csv']:\n",
    "    \n",
    "    with open('corpus/'+file, 'r', encoding=\"utf-8\") as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            line_=line.split(';')\n",
    "            data.append(line_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Verify codes that are in the train corpus but no more in ICD 10\n",
    "nocodes=[]\n",
    "old_codes=[]\n",
    "for d in data:\n",
    "    c=d[11].replace('\\n','')\n",
    "    c=re.sub(r'[\\+\\.\\-†*!\\s]','',c)\n",
    "    if len(c)>1 and c not in icd:\n",
    "   \n",
    "        nocodes.append(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text and codes\n",
    "\n",
    "For texts, use a specific tokenzier:\n",
    "- stop word removal (see dictionairies/stop_word.txt list)\n",
    "- punctuation removal\n",
    "- spliting words with various special caracter used by physicians\n",
    "- acronyms replacement (hand made non exhaustive list dictionnaries/cim_abv)\n",
    "- spliting composed words\n",
    "\n",
    "For ICD codes, remove all dot and other additionnal characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents={}\n",
    "prep_codes={}\n",
    "ids={}\n",
    "rej=[]\n",
    "for d in data:\n",
    "    \n",
    "    s=d[6].replace('?',' interrogation')\n",
    "       \n",
    "    c=d[11].replace('\\n','')\n",
    "    c=re.sub(r'[\\+\\.\\-†*!\\s]','',c)\n",
    "    \n",
    "    if d[0]+d[1]+d[5] not in prep_codes:\n",
    "        \n",
    "        prep_codes[d[0]+d[1]+d[5]]=[]\n",
    "        sents[d[0]+d[1]+d[5]]=tp.tokenizer(s)\n",
    "        ids[d[0]+d[1]+d[5]]=d[0]\n",
    "    \n",
    "    if len(c)>0:\n",
    "        prep_codes[d[0]+d[1]+d[5]]+=[c]\n",
    "        if c not in icd:\n",
    "                rej.append(d[0]+d[1]+d[5])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for s in sents.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_vocab, rev_corpus_vocab = build_vocabulary([s for s in sents.values()])\n",
    "## use tp.corrector_dict from fit_corrector to recognise spelling errors or new words\n",
    "missing_words=[w for w in corpus_vocab if w not in tp.corrector_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic word correction (cf class TextPreprocessing) :\n",
    "- Preselection of word candidates by finding words with a similar number of characters\n",
    "- Calculation of levenshtein distance\n",
    "- Final discrimination with frequencies in the dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_correction={}\n",
    "non_corr=[]\n",
    "for w in missing_words:\n",
    "     \n",
    "    w_=tp.best_correction(w)\n",
    "    if w_!=w:\n",
    "        word_correction[w]=w_\n",
    "    else:\n",
    "        non_corr.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of new word not in the dictionnary at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "757"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('word_correction.csv', 'w') as f:\n",
    "    writer = csv.writer(f,delimiter=';')\n",
    "    for s,sc in word_correction.items():\n",
    "        writer.writerow( (s,sc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents_={}\n",
    "for k,s in sents.items():\n",
    "    \n",
    "    ns=[]\n",
    "    for w in s:\n",
    "        if w in tp.corrector_dict:\n",
    "            #some composed words which were not well ortographied should have been replaced in the previous step\n",
    "            if w in comp_words:\n",
    "                ns=ns+re.split('-',rev_comp_words[w])\n",
    "            else:\n",
    "                ns.append(w)\n",
    "        else:\n",
    "            if w in word_correction:\n",
    "                w__=word_correction[w]\n",
    "                if w__ in tp.corrector_dict:\n",
    "                    #some composed words which were not well ortographied should have been replaced in the previous step\n",
    "                    if w__ in comp_words:\n",
    "                        ns=ns+re.split('-',rev_comp_words[w__])\n",
    "                    else:\n",
    "                        ns.append(w__)\n",
    "\n",
    "    sents_[k]=ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for s in sents_.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "rej_=set(rej)\n",
    "with open('corpus/clef2018_FinalTrain.csv', 'w', encoding=\"utf-8\") as f:\n",
    "    \n",
    "    writer = csv.writer(f,delimiter=';')\n",
    "    \n",
    "    for k,val in sents_.items():\n",
    "        if k not in rej_:\n",
    "            s=' '.join(val)\n",
    "            c=' '.join(prep_codes[k])\n",
    "            id_=ids[k]\n",
    "\n",
    "            writer.writerow( (s,c,id_) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368064"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
